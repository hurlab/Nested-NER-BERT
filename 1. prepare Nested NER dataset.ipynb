{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a243945d-940c-4679-b95e-8061bbd4a2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "def read_ann_file_content(ann_file_path):\n",
    "    ids, entity_types, positions, entity_texts = [], [], [], []\n",
    "    with open(ann_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 3:\n",
    "                id_, entity_info, entity_text = parts\n",
    "                entity_type, position = entity_info.split(' ', 1)\n",
    "                ids.append(id_)\n",
    "                entity_types.append(entity_type)\n",
    "                positions.append(position)\n",
    "                entity_texts.append(entity_text)\n",
    "    return pd.DataFrame({\n",
    "        'id': ids,\n",
    "        'entity_type': entity_types,\n",
    "        'position': positions,\n",
    "        'entity_text': entity_texts\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79d14bce-7ff1-4fb2-a507-763c8e326911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates in file 26145744_en.ann:\n",
      "     id entity_type position entity_text\n",
      "24  T51     ANATOMY  180 185       sinus\n",
      "30  T62     ANATOMY  377 382       sinus\n",
      "76  T73     ANATOMY  377 382       sinus\n",
      "77  T83     ANATOMY  180 185       sinus\n"
     ]
    }
   ],
   "source": [
    "# Paths setup\n",
    "input_folder_path = 'Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev/'\n",
    "for filename in os.listdir(input_folder_path):\n",
    "    if filename.endswith('.ann'):\n",
    "        base_filename = os.path.splitext(filename)[0]\n",
    "        ann_file_path = os.path.join(input_folder_path, filename)\n",
    "        df = read_ann_file_content(ann_file_path)\n",
    "        #print(df)\n",
    "        # Find duplicate rows\n",
    "        duplicates = df[df.duplicated(subset=['entity_type', 'position', 'entity_text'], keep=False)]\n",
    "        \n",
    "        if not duplicates.empty:\n",
    "            print(f\"Duplicates in file {filename}:\")\n",
    "            print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3c6951-d597-46a7-b941-3a7e15893d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891405ea-a919-448f-b41a-913b283c986e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16411512-cc8b-4e1e-bc68-d6c188e5c119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/annotation testing/26281196_en_S7.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/annotation testing/26845866_en_S13.csv\n"
     ]
    }
   ],
   "source": [
    "#Can not handle disjoint cases properly\n",
    "\n",
    "def parse_position(position_str):\n",
    "    return [tuple(map(int, pos.split())) for pos in position_str.split(';')]\n",
    "\n",
    "def tokenize_with_positions_and_sentences(text):\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    tokens_with_positions_and_sentences = []\n",
    "    current_sentence_id = 0\n",
    "    current_position = 0\n",
    "    for sentence in sentences:\n",
    "        tokens = re.findall(r'\\d+\\.\\d+|[^\\w\\s]|\\w+', sentence)\n",
    "        for token in tokens:\n",
    "            start_index = text.find(token, current_position)\n",
    "            end_index = start_index + len(token)\n",
    "            tokens_with_positions_and_sentences.append((token, start_index, end_index, current_sentence_id))\n",
    "            current_position = end_index\n",
    "        current_sentence_id += 1\n",
    "    return tokens_with_positions_and_sentences\n",
    "\n",
    "def data_annotation(ann_file_path, txt_file_path, destination_folder_path):\n",
    "    annotations = read_ann_file_content(ann_file_path)\n",
    "    annotations['parsed_position'] = annotations['position'].apply(parse_position)\n",
    "    annotations = annotations.explode('parsed_position').reset_index(drop=True)\n",
    "    annotations[['start', 'end']] = pd.DataFrame(annotations['parsed_position'].tolist(), index=annotations.index)\n",
    "    annotations['length'] = annotations['end'] - annotations['start']\n",
    "    annotations.sort_values(by=['length', 'start'], ascending=[False, True], inplace=True)\n",
    "\n",
    "    with open(txt_file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    tokens_with_positions_and_sentences = tokenize_with_positions_and_sentences(text)\n",
    "    df_tokens = pd.DataFrame(tokens_with_positions_and_sentences, columns=['Token', 'Start', 'End', 'SentenceID'])\n",
    "    for i in range(1, 9):\n",
    "        df_tokens[f'NNER_L{i}'] = 'O'\n",
    "\n",
    "    for _, annotation in annotations.iterrows():\n",
    "        entity_type = annotation['entity_type']\n",
    "        start, end = annotation['start'], annotation['end']\n",
    "        overlapping_levels = [level for level in range(1, 9) if any((start <= row['Start'] < end) and row[f'NNER_L{level}'] != 'O' for _, row in df_tokens.iterrows())]\n",
    "        level = min(set(range(1, 9)) - set(overlapping_levels)) if overlapping_levels else 1\n",
    "\n",
    "        for i, row in df_tokens.iterrows():\n",
    "            token_start, token_end = row['Start'], row['End']\n",
    "            if start <= token_start < end:\n",
    "                tag_prefix = 'B' if token_start == start else 'I'\n",
    "                df_tokens.at[i, f'NNER_L{level}'] = f'{tag_prefix}-{entity_type}'\n",
    "\n",
    "    filename_without_extension = os.path.splitext(os.path.basename(txt_file_path))[0]\n",
    "    df_tokens['ID'] = df_tokens['SentenceID'].apply(lambda x: f'{filename_without_extension}_S{x}')\n",
    "    df_tokens.drop('SentenceID', axis=1, inplace=True)\n",
    "    cols = ['ID'] + [col for col in df_tokens.columns if col != 'ID']\n",
    "    df_tokens = df_tokens[cols]\n",
    "\n",
    "    unique_ids = df_tokens['ID'].unique()\n",
    "    for uid in unique_ids:\n",
    "        df_filtered = df_tokens[df_tokens['ID'] == uid]\n",
    "        filename = os.path.join(destination_folder_path, f\"{uid}.csv\")\n",
    "        df_filtered.to_csv(filename, index=False)\n",
    "    print(f\"Saved {filename}\")\n",
    "\n",
    "# Paths setup\n",
    "input_folder_path = 'Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/annotation testing'\n",
    "destination_folder_path = 'Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/annotation testing'\n",
    "\n",
    "if not os.path.exists(destination_folder_path):\n",
    "    os.makedirs(destination_folder_path)\n",
    "\n",
    "for filename in os.listdir(input_folder_path):\n",
    "    if filename.endswith('.ann'):\n",
    "        base_filename = os.path.splitext(filename)[0]\n",
    "        ann_file_path = os.path.join(input_folder_path, filename)\n",
    "        txt_file_path = os.path.join(input_folder_path, base_filename + '.txt')\n",
    "        \n",
    "        if os.path.exists(txt_file_path):\n",
    "            data_annotation(ann_file_path, txt_file_path, destination_folder_path)\n",
    "        else:\n",
    "            print(f\"Text file for {filename} not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57881983-8d92-45a9-999b-cb0893a0763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Works good  ############################\n",
    "\n",
    "'''import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def parse_position(position_str):\n",
    "    return [tuple(map(int, pos.split())) for pos in position_str.split(';')]\n",
    "\n",
    "def tokenize_with_positions_and_sentences(text):\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    tokens_with_positions_and_sentences = []\n",
    "    current_sentence_id = 0\n",
    "    current_position = 0\n",
    "    for sentence in sentences:\n",
    "        tokens = re.findall(r'\\d+\\.\\d+|[^\\w\\s]|\\w+', sentence)\n",
    "        for token in tokens:\n",
    "            start_index = text.find(token, current_position)\n",
    "            end_index = start_index + len(token)\n",
    "            tokens_with_positions_and_sentences.append((token, start_index, end_index, current_sentence_id))\n",
    "            current_position = end_index\n",
    "        current_sentence_id += 1\n",
    "    return tokens_with_positions_and_sentences\n",
    "\n",
    "def data_annotation(ann_file_path, txt_file_path, destination_folder_path):\n",
    "    def read_ann_file_content(ann_file_path):\n",
    "        annotations = []\n",
    "        with open(ann_file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split('\\t')\n",
    "                entity_info = parts[1].split(' ')\n",
    "                entity_type = entity_info[0]\n",
    "                positions = ' '.join(entity_info[1:])\n",
    "                entity_text = parts[2]\n",
    "                annotations.append({'entity_type': entity_type, 'position': positions, 'text': entity_text})\n",
    "        return pd.DataFrame(annotations)\n",
    "\n",
    "    annotations = read_ann_file_content(ann_file_path)\n",
    "    annotations['parsed_position'] = annotations['position'].apply(parse_position)\n",
    "    annotations['start'] = annotations['parsed_position'].apply(lambda x: x[0][0])\n",
    "    annotations['end'] = annotations['parsed_position'].apply(lambda x: x[-1][1])\n",
    "    annotations['length'] = annotations['end'] - annotations['start']\n",
    "    annotations.sort_values(by=['length', 'start'], ascending=[False, True], inplace=True)\n",
    "\n",
    "    with open(txt_file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    tokens_with_positions_and_sentences = tokenize_with_positions_and_sentences(text)\n",
    "    df_tokens = pd.DataFrame(tokens_with_positions_and_sentences, columns=['Token', 'Start', 'End', 'SentenceID'])\n",
    "    for i in range(1, 9):\n",
    "        df_tokens[f'NNER_L{i}'] = 'O'\n",
    "\n",
    "    for _, annotation in annotations.iterrows():\n",
    "        entity_type = annotation['entity_type']\n",
    "        entity_positions = annotation['parsed_position']\n",
    "        is_first_segment = True\n",
    "\n",
    "        for start, end in entity_positions:\n",
    "            overlapping_levels = [level for level in range(1, 9) if any((start <= row['Start'] < end) and row[f'NNER_L{level}'] != 'O' for _, row in df_tokens.iterrows())]\n",
    "            level = min(set(range(1, 9)) - set(overlapping_levels)) if overlapping_levels else 1\n",
    "\n",
    "            for i, row in df_tokens.iterrows():\n",
    "                token_start, token_end = row['Start'], row['End']\n",
    "                if start <= token_start < end:\n",
    "                    tag_prefix = 'B' if is_first_segment and token_start == start else 'I'\n",
    "                    df_tokens.at[i, f'NNER_L{level}'] = f'{tag_prefix}-{entity_type}'\n",
    "            is_first_segment = False\n",
    "\n",
    "    filename_without_extension = os.path.splitext(os.path.basename(txt_file_path))[0]\n",
    "    df_tokens['ID'] = df_tokens['SentenceID'].apply(lambda x: f'{filename_without_extension}_S{x}')\n",
    "    df_tokens.drop('SentenceID', axis=1, inplace=True)\n",
    "    cols = ['ID'] + [col for col in df_tokens.columns if col != 'ID']\n",
    "    df_tokens = df_tokens[cols]\n",
    "\n",
    "    unique_ids = df_tokens['ID'].unique()\n",
    "    for uid in unique_ids:\n",
    "        df_filtered = df_tokens[df_tokens['ID'] == uid]\n",
    "        filename = os.path.join(destination_folder_path, f\"{uid}.csv\")\n",
    "        df_filtered.to_csv(filename, index=False)\n",
    "    print(f\"Saved {filename}\")\n",
    "\n",
    "# Paths setup\n",
    "input_folder_path = 'Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/annotation testing'\n",
    "destination_folder_path = 'Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/annotation testing'\n",
    "\n",
    "if not os.path.exists(destination_folder_path):\n",
    "    os.makedirs(destination_folder_path)\n",
    "\n",
    "for filename in os.listdir(input_folder_path):\n",
    "    if filename.endswith('.ann'):\n",
    "        base_filename = os.path.splitext(filename)[0]\n",
    "        ann_file_path = os.path.join(input_folder_path, filename)\n",
    "        txt_file_path = os.path.join(input_folder_path, base_filename + '.txt')\n",
    "        \n",
    "        if os.path.exists(txt_file_path):\n",
    "            data_annotation(ann_file_path, txt_file_path, destination_folder_path)\n",
    "        else:\n",
    "            print(f\"Text file for {filename} not found.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc482a-d58e-4966-9dd1-5acfdc892e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61b0eb3-8a63-423a-bd23-64c35b204749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504209f5-cbc6-4873-a905-763f267de7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ec007-1d90-4410-8d6b-a5bf7a46b2ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d708e8b4-a7ac-41b5-af92-7511112c78ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d96292-079c-4f7c-ac1e-b986336d9376",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\\n *********************  Process Complete ********************\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0035d9-be1c-49cc-b600-23da65bd6b35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3416d7e9-7063-4995-9b4c-1c256cdba652",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "def read_ann_file_content(ann_file_path):\n",
    "    ids, entity_types, positions, entity_texts = [], [], [], []\n",
    "    with open(ann_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 3:\n",
    "                id_, entity_info, entity_text = parts\n",
    "                entity_type, position = entity_info.split(' ', 1)\n",
    "                ids.append(id_)\n",
    "                entity_types.append(entity_type)\n",
    "                positions.append(position)\n",
    "                entity_texts.append(entity_text)\n",
    "    return pd.DataFrame({\n",
    "        'id': ids,\n",
    "        'entity_type': entity_types,\n",
    "        'position': positions,\n",
    "        'entity_text': entity_texts\n",
    "    })\n",
    "\n",
    "def parse_position(position_str):\n",
    "    return [tuple(map(int, pos.split())) for pos in position_str.split(';')]\n",
    "\n",
    "def tokenize_with_positions_and_sentences(text):\n",
    "    # Detect sentences (simple approach, can be improved with NLP tools)\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    tokens_with_positions_and_sentences = []\n",
    "    current_sentence_id = 0\n",
    "    current_position = 0\n",
    "    for sentence in sentences:\n",
    "        tokens = re.findall(r'\\d+\\.\\d+|[^\\w\\s]|\\w+', sentence)\n",
    "        for token in tokens:\n",
    "            start_index = text.find(token, current_position)\n",
    "            end_index = start_index + len(token)\n",
    "            tokens_with_positions_and_sentences.append((token, start_index, end_index, current_sentence_id))\n",
    "            current_position = end_index\n",
    "        current_sentence_id += 1\n",
    "    return tokens_with_positions_and_sentences\n",
    "\n",
    "def data_annotation(ann_file_path, txt_file_path, destination_folder_path):\n",
    "    annotations = read_ann_file_content(ann_file_path)\n",
    "    annotations['parsed_position'] = annotations['position'].apply(parse_position)\n",
    "    annotations = annotations.explode('parsed_position').reset_index(drop=True)\n",
    "    annotations[['start', 'end']] = pd.DataFrame(annotations['parsed_position'].tolist(), index=annotations.index)\n",
    "    annotations['length'] = annotations['end'] - annotations['start']\n",
    "    annotations.sort_values(by=['start', 'end'], inplace=True)\n",
    "\n",
    "    with open(txt_file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    tokens_with_positions_and_sentences = tokenize_with_positions_and_sentences(text)\n",
    "    df_tokens = pd.DataFrame(tokens_with_positions_and_sentences, columns=['Token', 'Start', 'End', 'SentenceID'])\n",
    "    for i in range(1, 9):\n",
    "        df_tokens[f'NNER_L{i}'] = 'O'\n",
    "\n",
    "    for _, annotation in annotations.iterrows():\n",
    "        entity_type = annotation['entity_type']\n",
    "        start, end = annotation['start'], annotation['end']\n",
    "        overlapping_levels = [level for level in range(1, 9) if any((start <= row['Start'] < end) for _, row in df_tokens.iterrows() if row[f'NNER_L{level}'] != 'O')]\n",
    "        level = min(set(range(1, 9)) - set(overlapping_levels)) if overlapping_levels else 1\n",
    "\n",
    "        for i, row in df_tokens.iterrows():\n",
    "            token_start, token_end = row['Start'], row['End']\n",
    "            if start <= token_start < end:\n",
    "                tag_prefix = 'B' if token_start == start else 'I'\n",
    "                df_tokens.at[i, f'NNER_L{level}'] = f'{tag_prefix}-{entity_type}'\n",
    "\n",
    "    filename_without_extension = os.path.splitext(os.path.basename(txt_file_path))[0]\n",
    "    df_tokens['ID'] = df_tokens['SentenceID'].apply(lambda x: f'{filename_without_extension}_S{x}')\n",
    "    df_tokens.drop('SentenceID', axis=1, inplace=True)\n",
    "    cols = ['ID'] + [col for col in df_tokens.columns if col != 'ID']\n",
    "    df_tokens = df_tokens[cols]\n",
    "\n",
    "    unique_ids = df_tokens['ID'].unique()\n",
    "    for uid in unique_ids:\n",
    "        df_filtered = df_tokens[df_tokens['ID'] == uid]\n",
    "        filename = os.path.join(destination_folder_path, f\"{uid}.csv\")\n",
    "        df_filtered.to_csv(filename, index=False)\n",
    "    print(f\"Saved {filename}\")\n",
    "\n",
    "# Paths setup\n",
    "input_folder_path = 'Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/annotation testing'\n",
    "destination_folder_path = 'Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/annotation testing'\n",
    "\n",
    "if not os.path.exists(destination_folder_path):\n",
    "    os.makedirs(destination_folder_path)\n",
    "\n",
    "for filename in os.listdir(input_folder_path):\n",
    "    if filename.endswith('.ann'):\n",
    "        base_filename = os.path.splitext(filename)[0]\n",
    "        ann_file_path = os.path.join(input_folder_path, filename)\n",
    "        txt_file_path = os.path.join(input_folder_path, base_filename + '.txt')\n",
    "        \n",
    "        if os.path.exists(txt_file_path):\n",
    "            data_annotation(ann_file_path, txt_file_path, destination_folder_path)\n",
    "        else:\n",
    "            print(f\"Text file for {filename} not found.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "192757ea-a8a3-4a15-a9b9-fc0595698e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26489117_en_S14.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/27030325_en_S10.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26525628_en_S6.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26356060_en_S12.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26977908_en_S6.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26525809_en_S5.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26485774_en_S12.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26978233_en_S8.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26356398_en_S13.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26525622_en_S7.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/25823274_en_S14.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26824818_en_S8.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26087637_en_S13.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26528618_en_S15.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26356526_en_S8.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26310014_en_S13.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/25909557_en_S9.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26031946_en_S8.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26146046_en_S11.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26525616_en_S6.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/25872388_en_S8.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26356616_en_S4.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26525625_en_S7.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/27029443_en_S5.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/25909608_en_S6.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/25909675_en_S7.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/25823273_en_S5.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26356163_en_S6.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26288207_en_S8.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26271422_en_S6.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26525473_en_S8.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26356515_en_S8.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26753198_en_S8.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26288285_en_S9.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/25726786_en_S9.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/25909792_en_S9.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26281191_en_S10.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26081321_en_S11.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26245101_en_S6.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/27022660_en_S12.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/25864346_en_S12.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/25868364_en_S10.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/25823275_en_S12.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26978498_en_S11.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26953424_en_S11.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/25868366_en_S8.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26978046_en_S11.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26145744_en_S9.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26355936_en_S7.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/26087638_en_S9.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths setup\n",
    "input_folder_path = 'Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev/'\n",
    "\n",
    "destination_folder_path = 'Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4'\n",
    "\n",
    "if not os.path.exists(destination_folder_path):\n",
    "    os.makedirs(destination_folder_path)\n",
    "\n",
    "for filename in os.listdir(input_folder_path):\n",
    "    if filename.endswith('.ann'):\n",
    "        base_filename = os.path.splitext(filename)[0]\n",
    "        ann_file_path = os.path.join(input_folder_path, filename)\n",
    "        txt_file_path = os.path.join(input_folder_path, base_filename + '.txt')\n",
    "        \n",
    "        if os.path.exists(txt_file_path):\n",
    "            data_annotation(ann_file_path, txt_file_path, destination_folder_path)\n",
    "        else:\n",
    "            print(f\"Text file for {filename} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3b6c0f2-53fb-4b6e-bccc-12855463f577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26281196_en_S7.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26591552_en_S10.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26120981_en_S6.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26245096_en_S5.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26356162_en_S9.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26978051_en_S12.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26977625_en_S6.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26356617_en_S6.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26355933_en_S6.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26977916_en_S7.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26390722_en_S13.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26356615_en_S10.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26978491_en_S13.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26027241_en_S16.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26485778_en_S10.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/27030431_en_S6.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26081328_en_S7.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26978499_en_S11.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26036067_en_S7.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26977723_en_S14.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26977615_en_S10.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26977764_en_S16.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26271558_en_S7.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26977789_en_S10.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26821419_en_S11.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26281190_en_S12.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26600613_en_S10.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26845866_en_S13.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26821417_en_S7.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26355937_en_S6.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/25591652_en_S16.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26525480_en_S10.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26569010_en_S8.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26977790_en_S10.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26549907_en_S8.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26978232_en_S9.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/27030091_en_S7.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26978639_en_S8.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26978176_en_S12.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26978635_en_S9.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26529621_en_S13.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/25842921_en_S12.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26978504_en_S12.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/27029445_en_S7.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26978172_en_S12.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/25823269_en_S15.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26226776_en_S7.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26036068_en_S8.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26081340_en_S8.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26081319_en_S10.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/27030332_en_S8.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/25842923_en_S7.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26978418_en_S18.csv\n",
      "Saved Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/26529536_en_S12.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths setup\n",
    "input_folder_path = 'Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train/'\n",
    "\n",
    "destination_folder_path = 'Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4'\n",
    "\n",
    "if not os.path.exists(destination_folder_path):\n",
    "    os.makedirs(destination_folder_path)\n",
    "\n",
    "for filename in os.listdir(input_folder_path):\n",
    "    if filename.endswith('.ann'):\n",
    "        base_filename = os.path.splitext(filename)[0]\n",
    "        ann_file_path = os.path.join(input_folder_path, filename)\n",
    "        txt_file_path = os.path.join(input_folder_path, base_filename + '.txt')\n",
    "        \n",
    "        if os.path.exists(txt_file_path):\n",
    "            data_annotation(ann_file_path, txt_file_path, destination_folder_path)\n",
    "        else:\n",
    "            print(f\"Text file for {filename} not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40a1df4-49db-487d-9fc8-261c4ab4dfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def find_empty_columns_in_csv(folder_path):\n",
    "    # Dictionary to store the filename and the number of empty columns\n",
    "    results = {}\n",
    "\n",
    "    # Iterate over all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Count the number of columns where all entries are \"O\"\n",
    "            empty_columns = 0\n",
    "            for column in df.columns:\n",
    "                if all(df[column] == 'O'):\n",
    "                    empty_columns += 1\n",
    "            \n",
    "            # Store the result in the dictionary\n",
    "            results[filename] = empty_columns\n",
    "    \n",
    "    # Find the file with the lowest number of empty columns\n",
    "    min_empty_columns = min(results.values())\n",
    "    for file, count in results.items():\n",
    "        if count == min_empty_columns:\n",
    "            print(f\"Filename: {file}, Empty Columns: {count}\")\n",
    "\n",
    "# Specify the path to the folder containing the CSV files\n",
    "folder_path = 'Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/'\n",
    "\n",
    "# Call the function\n",
    "find_empty_columns_in_csv(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952628f3-489a-4b89-8b9c-930a99cc5d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c6d19f9-2bd5-469b-bd14-e6dcd29a6f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: 26281196_en_S5.csv, Empty Columns: 3\n",
      "Filename: 26591552_en_S9.csv, Empty Columns: 3\n",
      "Filename: 26485778_en_S9.csv, Empty Columns: 3\n",
      "Filename: 26845866_en_S12.csv, Empty Columns: 3\n",
      "Filename: 26978635_en_S0.csv, Empty Columns: 3\n",
      "Filename: 25842921_en_S6.csv, Empty Columns: 3\n",
      "Filename: 27029445_en_S6.csv, Empty Columns: 3\n",
      "Filename: 26081319_en_S3.csv, Empty Columns: 3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def find_files_with_two_empty_columns(folder_path):\n",
    "    # List to store filenames with exactly two empty columns\n",
    "    files_with_two_empty_columns = []\n",
    "\n",
    "    # Iterate over all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Count the number of columns where all entries are \"O\"\n",
    "            empty_columns = 0\n",
    "            for column in df.columns:\n",
    "                if all(df[column] == 'O'):\n",
    "                    empty_columns += 1\n",
    "            \n",
    "            # Check if the file has exactly two empty columns\n",
    "            if empty_columns < 4:\n",
    "                files_with_two_empty_columns.append(filename)\n",
    "    \n",
    "    # Print the results\n",
    "    if files_with_two_empty_columns:\n",
    "        for file in files_with_two_empty_columns:\n",
    "            print(f\"Filename: {file}, Empty Columns: 3\")\n",
    "    else:\n",
    "        print(\"No files with exactly two empty columns found.\")\n",
    "\n",
    "# Specify the path to the folder containing the CSV files\n",
    "folder_path = 'Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/'\n",
    "\n",
    "# Call the function\n",
    "find_files_with_two_empty_columns(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85a495db-6a20-4a88-bfa9-8c47d17e0405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Dev:\n",
      "Filename: 27030325_en_S4.csv, Empty Columns: 2\n",
      "Filename: 27030325_en_S7.csv, Empty Columns: 2\n",
      "Filename: 26485774_en_S0.csv, Empty Columns: 2\n",
      "Filename: 26145744_en_S1.csv, Empty Columns: 2\n",
      "Filename: 26145744_en_S2.csv, Empty Columns: 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def print_empty_columns_for_each_file(folder_path):\n",
    "    # Iterate over all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Count the number of columns where all entries are \"O\"\n",
    "            empty_columns = 0\n",
    "            for column in df.columns:\n",
    "                if all(df[column] == 'O'):\n",
    "                    empty_columns += 1\n",
    "            \n",
    "            # Print the result for the current file\n",
    "            if empty_columns<=2:\n",
    "                print(f\"Filename: {filename}, Empty Columns: {empty_columns}\")\n",
    "\n",
    "print(\"Train:\")\n",
    "# Specify the path to the folder containing the CSV files\n",
    "folder_path = 'Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/train_Nested_entity_separated_V4/'\n",
    "\n",
    "# Call the function\n",
    "print_empty_columns_for_each_file(folder_path)\n",
    "\n",
    "\n",
    "print(\"Dev:\")\n",
    "# Specify the path to the folder containing the CSV files\n",
    "folder_path = 'Dataset/DATASET_BIONNE_UPDATED/DATASET_BIONNE/en/dev_Nested_entity_separated_V4/'\n",
    "\n",
    "# Call the function\n",
    "print_empty_columns_for_each_file(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940ca1bd-d32a-4915-b72b-dec39ed1cb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "Therefore number of nesting level is 6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaienv",
   "language": "python",
   "name": "openaienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
